To predict punctuations in unformatted text, we use two different models: a lexical model, which is based on lexical features, and an acoustic model, which is based on prosodic features.
For our approach we used a deep learning approach using the deep learning framework \emph{Caffe}\footnote{http://caffe.berkeleyvision.org/}.
In this chapter we describe how we predict the position of periods and commas in unpunctuated text using lexical features.

\subsection{Training instance generation}
For training our lexical model we took the \texttt{.xml} files of the TED talks and the Wikipedia files.
Before we can train the neural network with \emph{Caffe}, we have to preprocess the data to generate the training instances.
An overview of the individual steps from creating the training instances from the input files is shown in Figure~\ref{fig:overview_lexical}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{img/overview_lexical.pdf}
    \caption{Creation of the training instances from the data set}
    \label{fig:overview_lexical}
\end{figure}

In a first step the plain text from the input files is extracted.
The input text is formatted, i.e., it contains all kinds of punctuation marks.
We can use this information as our gold standard.
We use the following two classes: \textsc{Period} and \textsc{Comma}.
The class \textsc{Period} is mapped to the following characters: ``;'', ``.'', ``!'' and ``?''.
The characters ``,'', ``:'' and ``-'' are mapped to the class \textsc{Comma}.
If there is no punctuation, we use the class \textsc{None}.

Once we have the input text, the first step is the tokenization.
Afterwards, we run a sliding window over the input text to create the training instances.
The sliding window procedure is illustrated in Figure~\ref{fig:sliding_window}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{img/sliding_window.pdf}
    \caption{Training instances from sliding window: Each window constitutes one training instances. The length of the window and the position, which determines the label, is a parameter. In the example we use a window size of 5 and the punctuation position 3.}
    \label{fig:sliding_window}
\end{figure}
The size of the window is determined by a \texttt{config} file.
The \texttt{config} file stores different parameters, which are used during the process of generating the training instances.
Having those \texttt{config} files makes it easy to generate different kind of training instances and therefore to evaluate different configurations.
The window size directly defines the size of the training instance.
The label of a training instance is determined by whether there is a punctuation at a certain position or not.
This position is also defined in the \texttt{config} file and is called punctuation position.
If there is no punctuation at this position, the label of the training instance would be 0 (stands for class \textsc{None}), if there is one character of the class \textsc{Comma}, it would be 1, and if it is a character of the class \texttt{Period}, the label would be 2.

After we divide the sentences into training instances, we convert each word into a distributed word representation.
This step is necessary to obtain a representation of the words, which the neural network can learn from.
As implementation of the distributed representation, we use \emph{word2vec}\cite{Mikolov1, Mikolov2, Mikolov3}. % TODO: more?
Concretely, we used a pretrained vector\footnote{\url{https://code.google.com/archive/p/word2vec/}} file, which was trained on a Google News data set.
The model contains 300-dimensional word vectors for 3 million different words and phrases.
Since not every word exists in the trained model, we use the word vector representing \texttt{this} for unknown words.
We think that most of the unknown words are proper names, which can be replaced with \texttt{this} without changing the meaning and structure of the whole sentence.

We also figured out that the trained model only contains a few numbers.
As the concrete number occurring in a text is not relevant for the sentence boundary detection, we replace all numbers in the text, including floating-point numbers, with the number 1.
The representation for each word in an instance is inserted into one row of the feature matrix, e.g. for a sliding window size of 5 you get the matrix of 5x300 for each instance.

Besides the distributed word representation, we also examined Part of Speech (POS) tags as features.
Therefore, we used the \emph{nltk}\footnote{\url{http://www.nltk.org/}} POS tagger.
The tagger distinguishes between 35 different POS tags.
These are too specific for our purpose, so we grouped them into 14 different categories.
For example, there exist four different tags, which identify a noun.
The nltk tagger predicts more than one tag per word, so one word can have multiple tags (even after reducing the tag categories).
To allow the existence of multiple tags in our feature matrix, we used the following representation:
We create one flag per POS category which has the value 1 if the word belongs to this category and 0 otherwise.

One can think of several strategies to include the POS features in the training instances.
We decided for the most straightforward: appending the features after the word vector representation, thus resulting in a feature matrix of 5x314 for a sliding window size of 5 with POS tags.

% Data preparation
% Windowing
% Config.ini File

% Google Vector

\subsection{Neural Network Layout}

We use a neural network layout with three main \texttt{inner product} layers with sizes 2048, 4096, and 2048.
After each of these layers we added a \texttt{ReLU} and a \texttt{dropout} layer on top of each \texttt{inner product} layer.
The ratio for all \texttt{dropout} layers is $0.5$.
Accuracy and loss of the network are computed after the final predictions of our fourth \texttt{inner product} layer wih size 3.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{img/net_lexical.pdf}
    \caption{Network architecture consisting of four fully connected layers.}
    \label{fig:net_lexical}
\end{figure}

\subsection{Results and Evaluation}

We evaluated F-measure, which is based on precision \emph{P} and recall \emph{R} as seen in \ref{equ:f1n}.
The F-measure can be calculated for each class.
To calculate one number for the performance in all three classes, we use the generalized form of the F-measure, the harmonic mean.
The equation is shown in \ref{equ:f1}.
The higher the value, the better.
We use this value in the following diagrams.

\begin{equation}
\label{equ:f1n}
F1_{N} = 2 * \frac{P_{N}* R_{N}}{P_{N}+R_{N}}
\end{equation}

\begin{equation}
\label{equ:f1}
F = \frac{3}{\frac{1}{F1_{None}} + \frac{1}{F1_{Comma}} + \frac{1}{F1_{Period}}}
\end{equation}

All experiments are executed with the previously described data preprocessing and network layout.
As test data we used TED talks from 2011, which we did not include in the training set.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{img/window_eval.png}
    \caption{Harmonic mean between all F1 scores for all classes. \emph{2/5} means window size of five and punctuation position two. If \emph{wi} is in the label, it uses Wikipedia training data.}
    \label{fig:window_eval}
\end{figure}

In the experiments, we varied the window size and the punctuation position to find the best configuration.
In Figure \ref{fig:window_eval} you can see the results.
We consistently found out, that the best position for the punctuation is in the middle of the window.
We get the best results for a window size of 8.
This means that more context the better the results, until a certain point.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{img/window_wiki_eval.png}
    \caption{F score with different window and punctuation positions. Compares only TED talks and TED talks with additional Wikipedia data as training data.}
    \label{fig:window_wiki_eval}
\end{figure}

As described in Section \ref{sec:training_data}, we wanted to use more data for lexical prediction.
In Figure \ref{fig:window_wiki_eval} we show a comparison between using only TED talks and using additionally Wikipedia data as training data.
The described F-score is used, as well as different window sizes and punctuation positions.
The result for varying the window size is the same as described in the experiments above.
You can see that using Wikipedia data as additional training data always performs worse.

It seems that adding more training data in form of Wikipedia articles does not help to increase the precision when evaluating on TED talks.
Our hypothesis is, that the speech from TED talks is fundamentally different from the formal, written Wikipedia articles.
We can confirmed this by not evaluating on TED talks, but rather on different Wikipedia articles.
In that case, adding Wikipedia articles to the training set increased the F-measure by 6\%, overall yielding a F-score of 0.60.
For segmenting ASR output, we therefore do not use additional Wikipedia data for prediction.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{img/window_pos_eval.png}
    \caption{}
    \label{fig:window_pos_eval}
\end{figure}

We also evaluated whether POS tags improve the results.
Figure~\ref{fig:window_posBeval} shows the F-score depending on the different window sizes and punctuation positions.
The red bars show the results with using POS features and the blue bars show the result without.
Again, we see a consistent behaviour: POS tags always help increase the performance.

Since the punctuation types in the data sets are not equally distributed, we thought about reducing the dataset to a set where all three classes are balanced.
Therefore we created training sets with equally distributed punctuations types.
Experiments showed that this performs worse in all configurations.
The main reason is probably, that the data set size is much smaller.
