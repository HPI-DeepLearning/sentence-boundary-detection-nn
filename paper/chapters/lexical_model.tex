In this chapter we describe how we predict the position of periods, commas and question marks in unpunctuated text using lexical features.
The used data is described in Section \ref{sec:training_data}.

The system is based on the deep learning framework \emph{caffe}.
Therefore the processing pipeline is the following:

\begin{itemize}
\item preprocess the data, so that it can be used as input for caffe
\item train the network with the preprocessed data
\item retrieve predictions for new data
\item transform the output into a valuable result
\end{itemize}

\subsection{Data Preprocessing}

For processing the data, we need to extract features which caffe can handle.
Since you can not use a whole text as input, we decided to use a sliding window to generate training and testing instances.
A sliding window is in our case, that we split a sentence like \emph{The sun is shining and we go outside} into the following pieces (assuming a window size of 5):
\begin{itemize}
\item the sun is shining and
\item sun is shining and we
\item is shining and we go
\item shining and we go outside
\end{itemize}

We call these pieces instances and the window size directly defines how many words are in each instance.
The label of each instance is then whether there is a punctuation (and which one) at a defined position.
We call this position punctuation position.
In the above example every instance would have a label of \emph{None} with a punctuation position of 2, since there is no punctuation.

After we split up the sentences into instances we convert each word into a distributed word representation.
We use the distributed word representation described earlier: \emph{word2vec}. \todo{Insert word2vec link. Describe Word2Vec in related work?}
The final model we used for our distributed word representation was trained on google news entries \todo{Link and/or paper}.
Since not every word exists in the trained model, we use the word vector representing \emph{this} for unknown words.
We think that most of the unknown words are proper names, which can easily be replaced with \emph{this} without changing the meaning and structure of the whole sentence.

We also figured out that the trained model only contains the number \emph{1} and a few more of the most common numbers instead of all numbers. \todo{Check whether only 1 is in the word vector or whether numbers like 12 and 5 are also contained}
That is why we replace any number with \emph{1}.

The representation for each word in an instance is inserted into one row of the feature matrix, e.g. for a sliding window size of 5 you get the matrix of 5x300 for each instance.

Besides the distributed word representation we also examined Part of Speech (POS) tags as features.
We used the nltk POS tagger to identify the POS tags with the whole text as input.
The tagger distinguishes between 35 different POS tags.
These are too specific for our purpose, so we reduced them into 14 different categories.
For example we combined CD and LS to a more generic category \emph{numeral}.
The nltk tagger predicts more than one tag per word, so one word can have multiple tags (even after reducing the tag categories).
To allow the existence of multiple tags in our feature matrix, we used the following representation:
We create one flag per POS category which has the value 1 if the word belongs to this category and 0 otherwise.

There are two possibilities to include them.
On the one hand it is possible to append all flags at the end to word vector representations.
On the other hand an extra channel with the flags can be used.

For all further testing, wherever we used the POS-features, we decided to append the flags to the vector representation of the words.
Therefore the resulting feature matrix for a sliding window size of 5 with POS tags is 5x314.

% Data preparation
% Windowing
% Config.ini File

% Google Vector

\subsection{Neural Network Layout}

We use a neural network layout with three main \texttt{innerproduct} layers with sizes 2048, 4096, and 2048.
After each of these layers we added a \texttt{ReLU} and a \texttt{dropout} layer on top of each \texttt{innerproduct} layer.
The ratio for all \texttt{dropout} layers is $0.5$.
Accuracy and loss of the network are computed after the final predictions of our fourth \texttt{innerproduct} layer wih size 3 or 4 depending on the number of punctuation symbols.

\subsection{Results and Evaluation}

We used the measure of precision \emph{P} and recall \emph{R} as well as the F-Measure. We calculated the F measure per class as shown in Equation \ref{equ:f1n}.
We also calculated the harmonic mean \emph{F} for all classes (None, Comma, Period) (see Equation \ref{equ:f1}). This means the higher that value is the better is it. This score is used in the following diagrams.

\begin{equation}
\label{equ:f1n}
F1_{N} = 2 * \frac{P_{N}* R_{N}}{P_{N}+R_{N}}
\end{equation}

\begin{equation}
\label{equ:f1}
F = \frac{3}{\frac{1}{F1_{None}} + \frac{1}{F1_{Comma}} + \frac{1}{F1_{Period}}}
\end{equation}

All experiments are executed with the previous described data preprocessing and network layout.
We used TED talks as testing data. 

In Figure \ref{fig:window_eval} you can see the results. 
We varied the window and punctuation position. 
We found out that the best position for the punctuation in the middle of the window is.
We get the best results for a window size of 8. 
The more context the better the results.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{img/window_eval.png}
    \caption{Harmonic mean between all F1 scores for all classes. \emph{2/5} means window size of five and punctuation position two. If \emph{wi} is in the label, it uses wikipedia training data.}
    \label{fig:window_eval}
\end{figure}

In Figure \ref{fig:window_wiki_eval} a comparison between using only TED talks and using additionally wikipedia data as training data.
The described F-score is used, as well as different window sizes and punctuation positions. 
The result for varying of window size is the same as described in the experiments above. 
You can see that using wikipedia data as additional training data always performs worse.
This shows that more data does not help in general, although %\cite{}
 described that more data performs best.
This result can be explained with the used testing data. 
The trained network is evaluated with TED talks.
This is spoken language and wikipedia articles are written language by its nature. 
This result shows that written and spoken language is different at predicting punctuations for other language structures.
We evaluated the network also with wikipedia data (different from trainings data). The results are XXX.
This shows that just the test data structure yields to the different results.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{img/window_wiki_eval.png}
    \caption{F score with different window and punctuation positions. Compares only TED talks and TED talks with additional wikipedia data as training data.}
    \label{fig:window_wiki_eval}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{img/window_pos_eval.png}
    \caption{}
    \label{fig:window_pos_eval}
\end{figure}

Comparison between experiments with and without POS tagging (other than that, they have the same configurations):
\begin{itemize}
\item With POS tagging: 0.305
\item Without POS tagging: 0.275
\end{itemize}

Comparison between experiments with and without wikipedia data (other than that, they have the same configurations):
\begin{itemize}
\item Without wikipedia: 0.385
\item With wikipedia data: 0.252
\end{itemize}


Formeln f√ºr Precision, Recall, Harmonic Mean
Evaluation, wikipidia usage?, results with wikipedia test set
