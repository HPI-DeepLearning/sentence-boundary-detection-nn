
\subsection{Punctuation Prediction}
There are some researcher that focused on pure lexical based punctuation prediction.

Gravano, Jansche und Bacchiani used a simple language model with n-grams to detect punctiations. 
They differed the n-grams between 3 and 6 and evaluated the size of the trainings set. 
They mainly used news articles as data. They found out that about 5-grams and the most data they could get, performed best.
The result is a precision value for comma of 0.55 and for period of 0.65. 
The recall value is 0.68 (comma) and 0.62 (period) \cite{gravano09}. 


Another more advanced method is dynamic conditional random fields. 
Lu and Tou Ng used adoption of this method to predict punctuations in transcripts of dialogs in Chinese and English, which means many short sentences and more than usual questions. 
They were able to outperform hidden event language model \cite{Lu_betterpunctuation}.

\cite{DBLP:conf/interspeech/UeffingBV13} also used dynamic conditional fields. They evaluated this method with different features like language model scores, parse trees, dynamic sentence length and token n-grams. 
They discovered that the usefullness of the features differs to the used texts. If the text is more structured then the parse tree features are useful, for TED-Talks for example they did not help much.


Ein Abschnitt für lexical, eines für acoustic.
Welche Features?
Welchen Grundalgorithmus? (SVM, Decision Tree?)
Deren Ergebnisse.



\subsection{Distributed Representations}
word2vec
\subsection{Caffe}
