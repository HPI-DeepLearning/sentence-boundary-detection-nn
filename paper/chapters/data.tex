We use two different data sets for training and evaluating our SBD system.
The first dataset is a set of TED talks\footnote{\url{ted.com}} from 2011 until 2014.
The second dataset is plain text from Wikipedia\footnote{\url{en.wikipedia.org}}.
This was extracted from the English Wikipedia as of February 2014.

\paragraph{TED talks}
Our TED talk data set consists of 57 talks. For each talk we have the following data files:
\begin{itemize}
	\item \texttt{.xml} file: This file contains a manually created script of the talk.
	The text is formatted and serves as training data for the lexical model (ground truth).
	\item \texttt{.ctm} file: This is a time-marked speech input.
	It contains one word per line with the second the word was said in the talk and its duration.
    This is a typical output from an ASR system.
	Additionally, each sentence is labeled in the file, so that the data can be used for training of the acoustic model.
	\item \texttt{.sph} file: This file contains raw pulse code modulation (PCM) data.
	This data can be converted into .wav files.
\end{itemize}

\paragraph{Wikipedia}
We extracted the plain text from English Wikipedia articles.
We selected only those articles with more than 10,000 characters, assuming that these articles have gained lots of attention, and therefore provide a good textual quality.
When extracting the plain text, we discarded lists, headlines, tables etc., focusing only on paragraphs, so we can make sure, that only proper sentences are used for training.
In total we received around 3.5 million new training instances from the Wikipedia articles.